import hnswlib
import numpy as np
from sentence_transformers import SentenceTransformer
import json

# Ladda in chunk-mappningen
with open("data/index/chunk_mapping.json", "r", encoding="utf-8") as f:
    chunk_mapping = json.load(f)

# Ladda in embedding-modellen (samma som vid indexering)
model = SentenceTransformer("sentence-transformers/multi-qa-mpnet-base-dot-v1")

# Ladda in indexet
p = hnswlib.Index(space="cosine", dim=768)
p.load_index("data/index/ifk_index.bin")  # Byt ut om index-filen har annat namn

# Lista med testfr친gor f칬r olika datasets
test_questions = [
    "Hur m친nga g친nger har IFK G칬teborg vunnit Svenska Cupen?",  # Ska h칛mta rekord
    "Vilka tr칛nare har IFK G칬teborg haft?",  # Ska h칛mta tr칛nare
    "Hur m친nga matcher har IFK G칬teborg spelat i Allsvenskan?",  # Ska h칛mta matcher
    "Vem 칛r IFK G칬teborgs b칛sta m친lskytt genom tiderna?"  # Ska h칛mta spelare
]

# Loopa genom testfr친gorna och kolla retrieval
for question in test_questions:
    query_embedding = model.encode(question)
    query_embedding = np.array([query_embedding], dtype=np.float32)

    labels, distances = p.knn_query(query_embedding, k=5)
    retrieved_chunks = [chunk_mapping[str(i)] for i in labels[0]]

    print(f"\n游댍 Fr친ga: {question}")
    for i, chunk in enumerate(retrieved_chunks):
        print(f"Chunk {labels[0][i]}:\n{chunk[:300]}...\n")
