import os
import faiss
import numpy as np
import json
import requests
from sentence_transformers import SentenceTransformer

PROCESSED_DIR = "data/processed"
INDEX_PATH = "data/index/ifk_faiss_index.index"

# Ladda embedding-modellen
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Ladda Faiss-index
dim = 384  # Dimensionen p√• embeddings
index = faiss.read_index(INDEX_PATH)


# ‚úÖ Ladda alla chunkade JSON-filer i en lista
chunk_data = []
for filename in os.listdir(PROCESSED_DIR):
    if filename.endswith("_chunks.json"):
        with open(os.path.join(PROCESSED_DIR, filename), "r", encoding="utf-8") as f:
            chunk_data.extend(json.load(f))

def embed_text(text):
    """ Skapa en embedding av texten f√∂r fr√•gan """
    return model.encode(text, normalize_embeddings=True)

def search_index(query, top_k=10):
    """ S√∂ker i Faiss-index och h√§mtar chunkar fr√•n JSON-filerna """
    query_vector = embed_text(query)
    query_vector = np.array([query_vector]).astype("float32")

    # S√∂k i Faiss-indexet
    distances, indices = index.search(query_vector, top_k)

    print(f"üîé DEBUG: R√•data fr√•n Faiss")
    print(f"   - Distances: {distances}")
    print(f"   - Indices: {indices}")

    indices = [int(idx) for idx in indices[0]]  # Konvertera till int

    results = []
    for idx in indices:
        if idx < len(chunk_data):  
            chunk = chunk_data[idx]  
            chunk_text = chunk["text"] if "text" in chunk else json.dumps(chunk, ensure_ascii=False)
            print(f"üîπ Hittad chunk {idx}: {chunk_text[:400]}...") 
            results.append(chunk_text)

    if not results:
        print("‚ùå Inga relevanta chunks hittades!")

    return results

def ask_lm_studio(question, max_tokens=150, temperature=0.3):
    """ St√§ll en fr√•ga och skicka relevanta chunks till LM Studio """
    retrieved_chunks = search_index(question)

    
    if not retrieved_chunks:
        return "Jag vet inte."

    context = "\n".join(retrieved_chunks[:3])  # max 3 chunks

    print(f"üì§ Skickar prompt till LM Studio (f√∂rkortad version visas)...")

    prompt = f"""
    Du √§r en expert p√• IFK G√∂teborg.
    Anv√§nd **endast** den givna informationen f√∂r att svara p√• fr√•gan.  
    Om svaret inte finns i texten, s√§g exakt: "Jag vet inte."  
    Anv√§nd **exakta ord** fr√•n texten och spekulera aldrig.

    **Information:**  
    {context}

    **Fr√•ga:** {question}

    **Svara enbart med exakt information fr√•n texten.**  
    Exempel p√• format:  
    - "Enligt texten har Mikael Nilsson spelat flest matcher i Champions League: 44"  
    - "Jag vet inte."
    
    **Svar:**
    """


    response = requests.post(
        "http://localhost:1234/v1/completions",
        json={
            "model": "deepseek-r1-distill-qwen-7b",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature  
        },
    )

    try:
        return response.json()["choices"][0]["text"].strip()
    except Exception as e:
        return f"Fel vid anrop till LM Studio: {str(e)}"
